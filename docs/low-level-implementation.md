# Low-Level Implementation Guide

This document provides detailed information about the internal implementation of go-crablet, including database schema, SQL functions, and low-level architectural decisions.

## Table of Contents

1. [Database Schema](#database-schema)
2. [SQL Functions](#sql-functions)
3. [Query and Projection Implementation](#query-and-projection-implementation)
4. [Advisory Locks Implementation](#advisory-locks-implementation)
5. [Transaction Management](#transaction-management)
6. [Error Handling](#error-handling)
7. [Performance Considerations](#performance-considerations)

## Database Schema

### Events Table

The primary table that stores all events in the system:

```sql
CREATE TABLE events (
    type VARCHAR(64) NOT NULL,
    tags TEXT[] NOT NULL,
    data JSON NOT NULL,
    transaction_id xid8 NOT NULL,
    position BIGSERIAL NOT NULL PRIMARY KEY,
    occurred_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT chk_event_type_length CHECK (LENGTH(type) <= 64)
);

-- Indexes for performance
CREATE INDEX idx_events_transaction_position_btree ON events(transaction_id, position);
CREATE INDEX idx_events_tags ON events USING GIN(tags);
CREATE INDEX idx_events_type ON events(type);
```

**Key Design Decisions:**
- **`transaction_id`**: Uses PostgreSQL's built-in `xid8` type for transaction IDs (generated by `pg_current_xact_id()`)
- **`position`**: `BIGSERIAL` auto-incrementing position within each transaction
- **`tags`**: PostgreSQL TEXT[] array for efficient querying and indexing
- **`data`**: JSON for flexible event payload storage
- **`occurred_at`**: Business timestamp (when the event logically occurred)
- **Type constraint**: Maximum 64 characters for event type names

### Commands Table

Audit trail for all commands executed:

```sql
CREATE TABLE commands (
    transaction_id xid8 NOT NULL PRIMARY KEY,
    type VARCHAR(64) NOT NULL,
    data JSONB NOT NULL,
    metadata JSONB,
    occurred_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT CURRENT_TIMESTAMP
);
```

**Purpose:**
- **Audit Trail**: Track all commands for debugging and compliance
- **Correlation**: Link commands to their generated events via `transaction_id`
- **Metadata**: Store additional information about command execution

### Transaction ID Generation

The system uses PostgreSQL's built-in transaction ID mechanism:

```sql
-- Transaction IDs are generated automatically using pg_current_xact_id()
SELECT pg_current_xact_id();
```

**Key Characteristics:**
- **Built-in PostgreSQL**: Uses `xid8` type for transaction IDs
- **Automatic Generation**: No custom sequences table needed
- **Transaction-scoped**: Each PostgreSQL transaction gets a unique ID
- **Monotonic**: Transaction IDs are guaranteed to be monotonically increasing

## SQL Functions

### Core Append Functions

#### 1. `append_events_batch()` - Unconditional Append

```sql
CREATE OR REPLACE FUNCTION append_events_batch(
    p_types TEXT[],
    p_tags TEXT[], -- array of Postgres array literals as strings
    p_data JSONB[]
) RETURNS VOID AS $$
BEGIN
    -- Insert directly into events table using UNNEST for better performance
    INSERT INTO events (type, tags, data, transaction_id)
    SELECT 
        t.type,
        t.tag_string::TEXT[], -- Cast the array literal string to TEXT[]
        t.data,
        pg_current_xact_id() -- Use PostgreSQL's built-in transaction ID
    FROM UNNEST($1, $2, $3) AS t(type, tag_string, data);
END;
$$ LANGUAGE plpgsql;
```

#### 2. `append_events_with_condition()` - Conditional Append

```sql
CREATE OR REPLACE FUNCTION append_events_with_condition(
    p_types TEXT[],
    p_tags TEXT[], -- array of Postgres array literals as strings
    p_data JSONB[],
    p_condition JSONB DEFAULT NULL
) RETURNS JSONB AS $$
DECLARE
    fail_if_events_match JSONB;
    after_cursor JSONB;
    condition_result JSONB;
BEGIN
    -- Extract condition parameters
    IF p_condition IS NOT NULL THEN
        fail_if_events_match := p_condition->'fail_if_events_match';
        IF p_condition->'after_cursor' IS NOT NULL AND p_condition->'after_cursor' != 'null' THEN
            after_cursor := p_condition->'after_cursor';
        END IF;
    END IF;
    
    -- Check append conditions first
    condition_result := check_append_condition(fail_if_events_match, after_cursor);
    
    -- If conditions failed, return the failure status
    IF (condition_result->>'success')::boolean = false THEN
        RETURN condition_result;
    END IF;
    
    -- If conditions pass, insert events using UNNEST for all cases
    PERFORM append_events_batch(p_types, p_tags, p_data);
    
    -- Return success status
    RETURN jsonb_build_object(
        'success', true,
        'message', 'events appended successfully',
        'events_count', array_length(p_types, 1)
    );
END;
$$ LANGUAGE plpgsql;
```

#### 3. `append_events_with_advisory_locks()` - Advisory Lock Append

```sql
CREATE OR REPLACE FUNCTION append_events_with_advisory_locks(
    p_types TEXT[],
    p_tags TEXT[], -- array of Postgres array literals as strings for storage
    p_data JSONB[],
    p_lock_tags TEXT[], -- array of Postgres array literals as strings for advisory locks
    p_condition JSONB DEFAULT NULL,
    p_lock_timeout_ms INTEGER DEFAULT 5000 -- 5 second default timeout
) RETURNS JSONB AS $$
DECLARE
    fail_if_events_match JSONB;
    after_cursor JSONB;
    lock_keys TEXT[];
    tag_array TEXT[];
    lock_key TEXT;
    i INTEGER;
    lock_timeout_setting TEXT;
    condition_result JSONB;
BEGIN
    -- Set lock timeout for this transaction
    lock_timeout_setting := current_setting('lock_timeout', true);
    PERFORM set_config('lock_timeout', p_lock_timeout_ms::TEXT, false);
    
    -- Extract condition parameters
    IF p_condition IS NOT NULL THEN
        fail_if_events_match := p_condition->'fail_if_events_match';
        IF p_condition->'after_cursor' IS NOT NULL AND p_condition->'after_cursor' != 'null' THEN
            after_cursor := p_condition->'after_cursor';
        END IF;
    END IF;
    
    -- Process each event's lock tags to acquire advisory locks
    FOR i IN 1..array_length(p_lock_tags, 1) LOOP
        -- Parse the lock tags array string into actual array
        tag_array := p_lock_tags[i]::TEXT[];
        
        -- Acquire advisory locks for all lock keys (sorted to prevent deadlocks)
        IF array_length(tag_array, 1) > 0 THEN
            FOR lock_key IN SELECT unnest(tag_array ORDER BY tag_array) LOOP
                PERFORM pg_advisory_xact_lock(hashtext(lock_key));
            END LOOP;
        END IF;
    END LOOP;
    
    -- Check append conditions
    condition_result := check_append_condition(fail_if_events_match, after_cursor);
    
    -- If conditions failed, return the failure status
    IF (condition_result->>'success')::boolean = false THEN
        RETURN condition_result;
    END IF;
    
    -- If conditions pass, insert events using UNNEST for all cases
    PERFORM append_events_batch(p_types, p_tags, p_data);
    
    -- Return success status
    RETURN jsonb_build_object(
        'success', true,
        'message', 'events appended successfully with advisory locks',
        'events_count', array_length(p_types, 1)
    );
END;
$$ LANGUAGE plpgsql;
```

### Query and Projection Implementation

**Note**: go-crablet implements querying and projection using **Go code with channels and goroutines** rather than SQL functions. This approach provides better performance, memory efficiency, and flexibility than SQL-based streaming.

#### Query Operations

Querying is handled by the Go `Query()` and `QueryStream()` methods:

```go
// Batch query - returns all matching events at once
func (es *eventStore) Query(ctx context.Context, query Query, after *Cursor) ([]Event, error)

// Stream query - returns events through a channel for memory efficiency
func (es *eventStore) QueryStream(ctx context.Context, query Query, after *Cursor) (<-chan Event, error)
```

**Key Benefits of Go-based Querying:**
- **Memory Efficiency**: Streaming prevents loading large datasets into memory
- **Backpressure**: Go channels provide natural backpressure control
- **Flexibility**: Complex query logic can be implemented in Go
- **Performance**: Direct database queries with optimized SQL generation

#### Projection Operations

Projection is handled by the Go `Project()` and `ProjectStream()` methods:

```go
// Batch projection - returns final states after processing all events
func (es *eventStore) Project(ctx context.Context, projectors []StateProjector, after *Cursor) (map[string]any, AppendCondition, error)

// Stream projection - returns intermediate states through channels
func (es *eventStore) ProjectStream(ctx context.Context, projectors []StateProjector, after *Cursor) (<-chan map[string]any, <-chan AppendCondition, error)
```

**Key Benefits of Go-based Projection:**
- **State Management**: Complex state transitions handled in Go
- **Multiple Projectors**: Efficiently process multiple projections in parallel
- **Cursor Tracking**: Automatic cursor management for DCB concurrency control
- **Memory Optimization**: Streaming prevents memory exhaustion with large datasets

#### SQL Query Generation

Both query and projection operations use the same optimized SQL generation:

```go
func (es *eventStore) buildReadQuerySQL(query Query, after *Cursor, limit *int) (string, []interface{}, error)
```

This generates efficient SQL queries with:
- **Tag-based filtering** using PostgreSQL's `@>` operator
- **Cursor-based pagination** for proper event ordering
- **GIN index utilization** for fast tag queries
- **Transaction ID ordering** for true event ordering guarantees

#### Streaming Architecture

The streaming implementation uses Go's concurrency primitives for optimal performance:

```go
// QueryStream architecture
func (es *eventStore) QueryStream(ctx context.Context, query Query, after *Cursor) (<-chan Event, error) {
    // Create buffered channel for backpressure control
    eventChan := make(chan Event, es.config.StreamBuffer)
    
    // Start goroutine for database querying
    go func() {
        defer close(eventChan)
        
        // Execute database query with timeout
        rows, err := es.pool.Query(ctx, sqlQuery, args...)
        defer rows.Close()
        
        // Stream events through channel
        for rows.Next() {
            event := convertRowToEvent(row)
            select {
            case eventChan <- event:
            case <-ctx.Done():
                return // Handle cancellation
            }
        }
    }()
    
    return eventChan, nil
}
```

**Key Streaming Features:**
- **Buffered Channels**: Configurable buffer size for backpressure control
- **Context Cancellation**: Proper cleanup on timeout or cancellation
- **Goroutine Management**: Efficient concurrent processing
- **Memory Safety**: Events streamed one at a time to prevent memory exhaustion

## Advisory Locks Implementation

### Lock Key Generation

Lock keys are generated from event tags with the `lock:` prefix:

```go
// In Go code
for _, tag := range event.GetTags() {
    if strings.HasPrefix(tag.GetKey(), "lock:") {
        lockKey := strings.TrimPrefix(tag.GetKey(), "lock:")
        lockKeys = append(lockKeys, lockKey)
    }
}
```

### Lock Acquisition Strategy

```sql
-- In SQL function
FOR lock_key IN SELECT unnest(lock_tags[i]::TEXT[]) LOOP
    PERFORM pg_advisory_xact_lock(hashtext(lock_key));
END LOOP;
```

**Key Characteristics:**
- **Transaction-scoped**: Locks are automatically released when transaction commits/rolls back
- **Hash-based**: Uses `hashtext()` for consistent lock key generation
- **Ordered**: Locks are acquired in a consistent order to prevent deadlocks
- **Timeout-protected**: Uses `lock_timeout` setting to prevent indefinite waiting

### Lock Key Examples

```go
// Example lock keys
"course:CS101"           // Lock specific course
"student:student123"     // Lock specific student
"enrollment:CS101"       // Lock enrollment operations for course
"account:account456"     // Lock specific account
```

## Transaction Management

### Isolation Levels

The system supports three isolation levels:

```go
type IsolationLevel int

const (
    IsolationLevelReadCommitted IsolationLevel = iota
    IsolationLevelRepeatableRead
    IsolationLevelSerializable
)
```

**Default**: `ReadCommitted` for most operations

### Transaction Flow

1. **Begin Transaction**: Start with specified isolation level
2. **Acquire Locks**: If advisory locks are needed
3. **Validate Conditions**: Check append conditions
4. **Generate Transaction ID**: Update sequences table
5. **Insert Events**: Batch insert all events
6. **Insert Command**: Store command for audit trail
7. **Commit**: All changes become visible

### Timeout Management

```go
func (es *eventStore) withTimeout(ctx context.Context, defaultTimeoutMs int) (context.Context, context.CancelFunc) {
    if deadline, ok := ctx.Deadline(); ok {
        // Use caller's timeout
        return context.WithDeadline(context.Background(), deadline)
    }
    // Use default timeout
    return context.WithTimeout(context.Background(), time.Duration(defaultTimeoutMs)*time.Millisecond)
}
```

## Error Handling

### Custom Error Codes

```sql
-- Custom error codes for specific scenarios
DCB01: Append condition violated
DCB02: Lock acquisition timeout
DCB03: Invalid event data
```

### Error Types in Go

```go
type EventStoreError struct {
    Op  string
    Err error
}

type ValidationError struct {
    EventStoreError
    Field string
    Value string
}

type ConcurrencyError struct {
    EventStoreError
}

type ResourceError struct {
    EventStoreError
    Resource string
}
```

### Error Recovery

- **Validation Errors**: Fail fast, no database changes
- **Concurrency Errors**: Retry with exponential backoff
- **Resource Errors**: Check database connectivity and configuration
- **Lock Timeouts**: Increase timeout or reduce concurrency

## Performance Considerations

### Indexing Strategy

```sql
-- Primary query patterns
CREATE INDEX idx_events_type ON events(type);
CREATE INDEX idx_events_transaction_id ON events(transaction_id);
CREATE INDEX idx_events_occurred_at ON events(occurred_at);

-- Tag-based queries
CREATE INDEX idx_events_tags_gin ON events USING GIN(tags);

-- JSON data queries
CREATE INDEX idx_events_data_gin ON events USING GIN(data);

-- Cursor-based pagination
CREATE UNIQUE INDEX idx_events_transaction_position ON events(transaction_id, position);
```

### Batch Operations

- **Batch Size Limit**: Configurable via `MaxBatchSize` (default: 1000)
- **Array Parameters**: Use PostgreSQL arrays for efficient batch inserts
- **Transaction Scope**: All events in a batch share the same transaction ID

### Lock Performance

- **Hash-based Keys**: Fast lock key generation and comparison
- **Transaction-scoped**: No manual lock cleanup required
- **Timeout Protection**: Prevents indefinite waiting
- **Ordered Acquisition**: Prevents deadlocks

### Query Optimization

- **Cursor-based Pagination**: Efficient for large datasets
- **Tag-based Filtering**: Uses GIN indexes for fast array operations
- **JSONB Queries**: Leverages PostgreSQL's JSONB indexing
- **Limit Clauses**: Prevents memory exhaustion

## Monitoring and Debugging

### Key Metrics

```sql
-- Event throughput
SELECT 
    DATE_TRUNC('hour', occurred_at) as hour,
    COUNT(*) as event_count,
    COUNT(DISTINCT transaction_id) as transaction_count
FROM events 
WHERE occurred_at >= NOW() - INTERVAL '24 hours'
GROUP BY hour
ORDER BY hour;

-- Lock contention
SELECT 
    locktype,
    database,
    relation,
    page,
    tuple,
    virtualxid,
    transactionid,
    classid,
    objid,
    objsubid,
    virtualtransaction,
    pid,
    mode,
    granted
FROM pg_locks 
WHERE locktype = 'advisory';

-- Transaction distribution
SELECT 
    COUNT(*) as event_count,
    COUNT(DISTINCT transaction_id) as transaction_count,
    AVG(events_per_transaction) as avg_events_per_tx
FROM (
    SELECT transaction_id, COUNT(*) as events_per_transaction
    FROM events 
    GROUP BY transaction_id
) tx_stats;
```

### Debug Queries

```sql
-- View recent events with full details
SELECT 
    e.*,
    c.type as command_type,
    c.data as command_data
FROM events e
LEFT JOIN commands c ON e.transaction_id = c.transaction_id
WHERE e.occurred_at >= NOW() - INTERVAL '1 hour'
ORDER BY e.transaction_id DESC, e.position DESC
LIMIT 100;

-- Check for duplicate transaction IDs (should never happen)
SELECT transaction_id, COUNT(*) as count
FROM events 
GROUP BY transaction_id 
HAVING COUNT(*) > 1;

-- Find events with specific tags
SELECT * FROM events 
WHERE tags @> ARRAY['course_id:CS101', 'student_id:student123']
ORDER BY occurred_at DESC;
```

This low-level documentation provides the foundation for understanding how go-crablet works internally, enabling developers to optimize, debug, and extend the system effectively. 
This low-level documentation provides the foundation for understanding how go-crablet works internally, enabling developers to optimize, debug, and extend the system effectively. 