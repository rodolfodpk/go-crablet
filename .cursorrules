# Cursor Project Rules for go-crablet

# Go-Crablet Cursor Configuration

## Project Overview
This is a Go event sourcing library **exploring** the Dynamic Consistency Boundary (DCB) approach with PostgreSQL for production and SQLite for benchmark test data.

**IMPORTANT: This is an exploration project, not a production-ready solution.**
- We are **learning and experimenting** with DCB concepts
- Performance claims should be **modest and factual**
- Emphasize **exploration** over **production readiness**
- Be **honest about limitations** and areas for improvement

## Comprehensive Testing Requirements

### Test Categories
- **Internal Tests**: Core DCB package tests (`pkg/dcb/`)
- **External Tests**: DCB test suite (`pkg/dcb/tests/`) and example applications (`internal/examples/`)
- **Benchmark Tests**: Performance validation (`internal/benchmarks/`)

### Mandatory Test Execution
**ALWAYS run both internal and external tests before considering any changes complete:**

```bash
# Run all internal tests (core DCB functionality)
go test ./pkg/dcb -v

# Run all external tests (DCB test suite and examples)
go test ./pkg/dcb/tests -v
go test ./internal/examples/... -v

# Run all tests comprehensively
go test ./... -v
```

### Test Validation Checklist
Before marking any task complete:
- [ ] Internal DCB tests pass (core functionality)
- [ ] External DCB test suite passes (advanced scenarios)
- [ ] External example tests pass (integration scenarios)
- [ ] All Ginkgo BDD tests pass (if applicable)
- [ ] No test failures or skipped tests
- [ ] Testcontainers integration working
- [ ] Database schema and functions created successfully

### Test Framework Requirements
- **Internal Tests**: Use Ginkgo v2 + Gomega for BDD testing
- **External Tests**: Support both standard Go tests and Ginkgo BDD
- **Database Tests**: Always use Testcontainers for isolated PostgreSQL instances
- **Coverage**: Maintain comprehensive test coverage for all public APIs

## Code Coverage System

### Coverage Calculation
The project uses a sophisticated coverage system with **gocovmerge** to combine coverage from multiple test suites:

1. **Internal Tests** (`pkg/dcb/`): ~15% coverage (31 Ginkgo specs)
2. **External Tests** (`pkg/dcb/tests/`): ~77% coverage (154 Ginkgo specs)
3. **Combined Coverage**: ~81% (using gocovmerge to merge coverage files)

### Coverage Scripts
- `scripts/generate-coverage.sh` - Generates comprehensive coverage using gocovmerge
- `scripts/update-coverage-badge.sh` - Updates README badge with coverage percentage
- GitHub Actions workflow (`.github/workflows/coverage.yml`) - Automated coverage reporting

### Coverage Workflow
```bash
# Generate comprehensive coverage
./scripts/generate-coverage.sh

# Update badge (optional)
./scripts/generate-coverage.sh update-badge
```

### Key Functions with Low Coverage (0%)
- `isConcurrencyError` (append.go:267) - Private helper, tested indirectly
- `NewEventStoreWithConfig` (constructors.go:40) - Used in docs/examples, tested through main constructor
- `NewCommandSimple` (constructors.go:216) - Simple wrapper, tested through NewCommand
- `IsTableStructureError` (errors.go:83) - Public API, may need tests if used
- `GetTableStructureError` (errors.go:120) - Public API, may need tests if used
- `AsConcurrencyError`, `AsResourceError`, `AsTableStructureError` (errors.go) - Simple aliases, tested through Get* functions
- `isInputEvent`, `isTag`, `isQuery`, `isQueryItem`, `isAppendCondition` (types.go) - Interface markers, no implementation to test

### Coverage Accuracy
The 80% coverage is legitimate because:
- **External tests are comprehensive** - they test main public APIs extensively
- **gocovmerge combines best coverage** - if a line is covered in either test suite, it counts
- **Core functionality is well-tested** - main EventStore, Append, Query, and Projection methods have good coverage
- **Low-coverage functions are edge cases** - many 0% functions are helper methods or alternative constructors

### Coverage Improvement Strategy
To improve coverage further:
1. **Add tests for 0% coverage functions** - especially error handling and alternative constructors
2. **Test edge cases** - concurrency errors, table structure errors, etc.
3. **Add integration tests** - for the config-based constructors
4. **Test helper functions** - the `is*` type checking functions

## Benchmark System

### Current Benchmark Structure
The project has a comprehensive benchmark system with two dataset sizes:

#### Dataset Sizes
- **"tiny"**: 5 courses, 10 students, 17 enrollments
  - **Use Case**: Quick testing, development, fast feedback
  - **Past Events**: 10 events for AppendIf testing
  - **Performance**: Best case scenarios, minimal data volume
- **"small"**: 1,000 courses, 10,000 students, 49,871 enrollments
  - **Use Case**: Realistic testing, production planning, scalability analysis
  - **Past Events**: 100 events for AppendIf testing
  - **Performance**: Real-world scenarios, data volume impact

#### Benchmark Categories
- **Core Operations**: Append, AppendIf, Read, Projection
- **Concurrent Scaling**: 1, 10, and 100 concurrent users
- **Realistic Batch Sizes**: 1, 5, and 12 events per operation
- **Business Scenarios**: Course enrollment, account transfers, mixed workflows

### Benchmark Documentation Structure
Performance documentation is organized into three files:

1. **`docs/performance.md`**: Overview with links to dataset-specific pages
2. **`docs/performance-tiny.md`**: Detailed results for Tiny dataset
3. **`docs/performance-small.md`**: Detailed results for Small dataset

### Running Benchmarks
```bash
cd internal/benchmarks
go test -bench=. -benchmem -benchtime=2s -timeout=5m .

# Quick tests
go test -bench=BenchmarkAppend_Tiny -benchtime=1s
```

### Benchmark Guidelines
- **Use realistic batch sizes**: 1-12 events per operation (not 100, 1000)
- **Test concurrency scaling**: 1, 10, and 100 users
- **Include both datasets**: Tiny for development, Small for production planning
- **Document performance insights**: Explain why performance differs between datasets
- **Show business context**: Explain what each benchmark scenario represents

## Documentation Structure

### Performance Documentation
- **Main overview** (`docs/performance.md`): High-level summary with navigation links
- **Dataset-specific pages**: Separate detailed results for each dataset size
- **Navigation links**: Easy browsing between overview and detailed results
- **Performance insights**: Clear explanations of performance characteristics
- **Dataset comparison**: Side-by-side comparison of Tiny vs Small datasets

### Documentation Guidelines
- **Be concise**: Remove verbose explanations and focus on key information
- **Use clear language**: Avoid jargon and invented terms
- **Organize by dataset**: Separate performance data by dataset size for clarity
- **Include navigation**: Add links between related documentation pages
- **Maintain consistency**: Use consistent terminology and formatting

### Documentation Files
- **`docs/overview.md`**: High-level concepts and API overview
- **`docs/performance.md`**: Performance guide overview with dataset links
- **`docs/performance-tiny.md`**: Tiny dataset detailed performance
- **`docs/performance-small.md`**: Small dataset detailed performance
- **`docs/command-execution-flow.md`**: Command execution workflow
- **`docs/examples.md`**: Code examples and usage patterns

## Terminology Guidelines

### DCB References
- **CORRECT**: "DCB approach", "DCB concepts", "DCB concurrency control"
- **INCORRECT**: "DCB pattern", "DCB patterns"
- **Reason**: DCB is an approach to thinking about consistency boundaries, not a design pattern

### CommandExecutor References
- **CORRECT**: "CommandExecutor API", "CommandExecutor convenience layer"
- **INCORRECT**: "CommandExecutor pattern", "Atomic command execution"
- **Reason**: CommandExecutor is an optional API helper, not a pattern, and uses database transactions for consistency

### Performance Claims
- **CORRECT**: "Performance varies by dataset size", "Tiny dataset shows best-case performance"
- **INCORRECT**: "15x faster than X", "Best performance in class"
- **Reason**: This is an exploration project, not a production solution

### Event Generation
- **CORRECT**: "Command handlers apply business logic to generate events"
- **INCORRECT**: "Commands are converted to events", "Automatic event generation"
- **Reason**: Business logic in handlers decides what events to create

## SQLite Test Data System

### Key Directories
- `internal/benchmarks/setup/` - Dataset generation and SQLite caching
- `internal/benchmarks/tools/` - Dataset preparation tools
- `cache/` - SQLite database with pre-generated test datasets
- `internal/benchmarks/benchmarks/` - Go benchmark tests

### SQLite Test Data Workflow

1. **Generate Test Datasets**:
   ```bash
   cd internal/benchmarks/tools
   go run prepare_datasets_main.go
   ```
   This creates SQLite cache with "tiny" and "small" datasets.

2. **Run Go Benchmarks**:
   ```bash
   make benchmark-go
   # or
   cd internal/benchmarks/benchmarks
   go test -bench=. -benchmem -benchtime=2s -timeout=5m .
   ```

3. **Clear Cache** (if needed):
   ```bash
   rm -rf cache/
   ```

### Dataset Sizes
- **"tiny"**: 5 courses, 10 students, 20 enrollments
- **"small"**: 1,000 courses, 10,000 students, 50,000 enrollments

### Important Notes
- SQLite is ONLY used for benchmark test data caching
- API consumers only see PostgreSQL dependency
- Benchmarks use cached datasets for fast execution
- No expensive dataset regeneration during benchmarks

### Database Setup
- PostgreSQL: Production database (localhost:5432/crablet)
- SQLite: Benchmark cache (cache/benchmark_datasets.db)

### Common Commands
- `make benchmark-go` - Run Go library benchmarks
- `make benchmark-all` - Run all benchmarks (web-app + Go)
- `make test` - Run unit tests
- `make build` - Build all packages

## Testing Stack

### Test Framework
- **Ginkgo v2**: BDD testing framework for Go
- **Gomega**: Matcher library for assertions
- **Testcontainers**: Containerized test dependencies
- **Docker Compose**: Local development environment

### Test Structure
```go
// Example test structure
var _ = Describe("EventStore", func() {
    var (
        ctx    context.Context
        store  dcb.EventStore
        pool   *pgxpool.Pool
    )

    BeforeEach(func() {
        ctx = context.Background()
        // Setup test containers or local PostgreSQL
    })

    AfterEach(func() {
        // Cleanup
    })

    Describe("Append", func() {
        It("should append events successfully", func() {
            // Test implementation
            Expect(err).To(BeNil())
            Expect(events).To(HaveLen(1))
        })
    })
})
```

### Running Tests
```bash
# Run all tests
make test

# Run tests with coverage
make test-coverage

# Run comprehensive coverage
make coverage

# Run specific test package
go test -v ./pkg/dcb/tests/...
```

## Development Guidelines
- Keep SQLite usage internal to benchmarks only
- Use PostgreSQL for all production code
- Cache datasets to avoid regeneration overhead
- Run benchmarks with reasonable timeouts (2-5 minutes)
- **Use Ginkgo + Gomega for all new tests**
- **Use Testcontainers for database dependencies**
- **Follow BDD style with Describe/Context/It blocks**
- **Write comprehensive assertions with Gomega matchers**

## Communication Guidelines
- **Always be modest** about performance and capabilities
- **Emphasize exploration** of DCB concepts, not production readiness
- **Be honest about limitations** and areas that need improvement
- **Avoid overstating** performance claims or system capabilities
- **Present this as a learning project** rather than a finished solution
- **Acknowledge that DCB is still evolving** and we're exploring its application

## Command Execution Terminology
- **NEVER say "Convert commands to events"** - this is misleading and incorrect
- **ALWAYS say "Execute command handlers to generate events"** or similar
- **Emphasize that command handlers apply business logic** to decide what events to create
- **Clarify there is no automatic conversion** - it's a deliberate business decision
- **The command handler is where business logic lives** and decides what events to create
- **Use phrases like**:
  - "Execute command logic to generate events"
  - "Command handler returns events"
  - "Process command and create events"
  - "Command execution produces events"
  - "Handler applies business logic and generates events"

## Critical Approval Requirements
- **ALL changes to `docker-entrypoint-initdb.d/schema.sql` require user approval**
- **ALL changes to core API in `pkg/dcb/` require user approval**
- **Database schema changes must be reviewed before implementation**
- **API breaking changes must be explicitly approved**
- **Never modify production database schema without approval** 

# Large File Policy
- **Never commit large binary files or build artifacts** (e.g., Go binaries, SQLite databases, large JSON/CSV, or benchmark result files) to git. These files bloat the repository, slow down operations, and make collaboration difficult.
- **Keep git history clean**: If large files are accidentally committed, use tools like `git filter-repo` to remove them from the entire history.
- **Always add large files and build artifacts to `.gitignore`** to prevent accidental commits.
- **Review `.gitignore` regularly** to ensure new large files or patterns are excluded.
- **If in doubt, ask before committing any file over 1MB**. 

# Cursor Project Rule: Event Tag Array Contract

- The contract for event tags in batch appends is:
  - Each event's tags are passed as a single Postgres array-literal string (e.g., '{"key1:value1","key2:value2"}').
  - The Go code (using pgx) must pass a []string where each element is a valid array-literal string for one event's tags.
  - The SQL function must accept TEXT[] (not TEXT[][]), and cast each element to TEXT[] inside the function.
  - The function must process, filter, and reconstruct tags as a valid array-literal string for each event, not as a comma-separated string or a nested array.

- Never require Go code to pass [][]string or Postgres text[][] for tags. Always use the array-literal string contract. 